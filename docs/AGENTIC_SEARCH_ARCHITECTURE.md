# ğŸ¤– Agentic Search Architecture

**Status**: ğŸ¯ **HIGHEST PRIORITY** - Core Feature  
**Version**: 1.0  
**Last Updated**: 2025-11-05

---

## ğŸ“‹ Executive Summary

Agentic Search is an intelligent, iterative search system that combines local knowledge (Qdrant), web search (SearXNG), selective crawling (Crawl4AI), and LLM-based decision making to provide comprehensive, high-quality answers to user queries.

**Key Innovation**: Unlike traditional search-then-crawl approaches, Agentic Search uses LLM evaluation at each stage to determine:
- Is local knowledge sufficient?
- Which URLs are worth crawling?
- What information gaps remain?
- How to refine queries for better results?

---

## ğŸ¯ Goals

### Primary Goals
1. **Maximize Answer Quality**: Provide complete, accurate answers by iteratively filling knowledge gaps
2. **Minimize Costs**: Only crawl URLs that LLM deems relevant (selective crawling)
3. **Leverage Existing Knowledge**: Check Qdrant first before hitting the web
4. **Iterative Refinement**: Automatically refine queries when initial results are insufficient

### Success Metrics
- **Completeness Score**: >80% answer completeness (LLM-evaluated)
- **Crawl Efficiency**: <30% of search results crawled (vs 100% in traditional approach)
- **Cost Reduction**: 50-70% fewer crawled pages vs exhaustive crawling
- **User Satisfaction**: Comprehensive answers in 1-3 iterations

---

## ğŸ—ï¸ High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      AGENTIC SEARCH PIPELINE                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  User Query                                                     â”‚
â”‚      â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ STAGE 1: Local Knowledge Check                           â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 1. Query Qdrant (with all RAG enhancements)          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Contextual embeddings (if enabled)              â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Hybrid search (if enabled)                      â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Reranking (if enabled)                          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 2. LLM Evaluation: Completeness Assessment           â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Input: Query + RAG results                        â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Output: {                                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      score: 0.0-1.0,                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      reasoning: "...",                               â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      gaps: ["missing X", "unclear Y"]                â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    }                                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 3. Decision:                                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    if score >= threshold (default 0.8):              â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Return results âœ…                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    else:                                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Go to STAGE 2                                 â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ STAGE 2: Web Search                                      â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 1. SearXNG Search                                    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â†’ Get URLs with titles/snippets                   â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 2. LLM Ranking: URL Relevance Assessment             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Input: Query + gaps + URL metadata               â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Output: [                                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      {url, title, score: 0.0-1.0, reasoning},       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      ...                                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    ]                                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 3. Filter Promising URLs                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â†’ Keep only score > 0.7                           â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â†’ Limit to top N (default 3)                      â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 4. Decision:                                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    if no promising URLs:                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Go to STAGE 4 (query refinement)             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    else:                                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Go to STAGE 3                                 â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ STAGE 3: Selective Crawling & Indexing                   â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 1. Crawl Promising URLs                              â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    scrape_urls(promising_urls)                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â†’ Crawl4AI extracts content                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 2. Full Indexing Pipeline                            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚ a. Smart Chunking                          â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Respect code blocks                   â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Respect paragraphs                    â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Configurable chunk_size               â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚                                            â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚ b. Contextual Embeddings (if enabled)      â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - LLM generates context for each chunk  â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Parallel processing (ThreadPool)      â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Fallback to standard on error         â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚                                            â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚ c. Embedding Generation                    â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - OpenAI text-embedding-3-small         â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Batch processing (20 chunks/batch)    â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚                                            â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚ d. Qdrant Storage                          â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Store chunks with embeddings          â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Add metadata (url, chunk_number, etc) â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â”‚    - Update sources table                  â”‚    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 3. Extract Crawl Metadata (RESEARCH NEEDED)          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    âš ï¸ TODO: Investigate Crawl4AI capabilities        â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Check if Crawl4AI returns content summaries     â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Check if metadata extraction is available       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Explore CrawlResult object structure            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Potential metadata to extract:                    â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Page title, description                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Main topics/keywords                            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Content structure (headers)                     â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    - Code blocks count/languages                     â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Use case: Pass to LLM for smarter Qdrant queries  â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 4. Generate Search Hints (Optional Enhancement)      â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    If metadata available:                            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      LLM: Generate optimal Qdrant queries            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      Input: Original query + crawled metadata        â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      Output: [                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚        "specific query 1",                           â”‚ â”‚  â”‚
â”‚  â”‚ â”‚        "specific query 2"                            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      ]                                               â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 5. Re-query Qdrant                                   â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    perform_rag_query(original_query)                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    OR (if hints available):                          â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    for hint in hints:                                â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      perform_rag_query(hint)                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 6. LLM Evaluation: Re-assess Completeness            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    (Same as STAGE 1)                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 7. Decision:                                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    if score >= threshold:                            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Return results âœ…                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    else:                                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Go to STAGE 4                                 â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚      â†“                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ STAGE 4: Query Refinement & Iteration                    â”‚  â”‚
â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚
â”‚  â”‚ â”‚ 1. LLM: Generate Refined Queries                     â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Input: Original query + current query + gaps      â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    Output: [                                         â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      "refined query 1",                              â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      "refined query 2"                               â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    ]                                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 2. Select Next Query                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    current_query = refined_queries[0]                â”‚ â”‚  â”‚
â”‚  â”‚ â”‚                                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚ 3. Iteration Control                                 â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    iteration++                                       â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    if iteration < max_iterations (default 3):        â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Go back to STAGE 2                            â”‚ â”‚  â”‚
â”‚  â”‚ â”‚    else:                                             â”‚ â”‚  â”‚
â”‚  â”‚ â”‚      â†’ Return best available results                 â”‚ â”‚  â”‚
â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”§ Component Details

### 1. Local Knowledge Check (STAGE 1)

**Purpose**: Leverage existing indexed content before hitting the web

**Components**:
- **Qdrant Query**: Uses existing `perform_rag_query` with all enabled enhancements
  - Respects `USE_CONTEXTUAL_EMBEDDINGS` flag
  - Respects `USE_HYBRID_SEARCH` flag
  - Respects `USE_RERANKING` flag
  - Respects `USE_AGENTIC_RAG` flag (for code search)

- **LLM Completeness Evaluator**:
  - Model: Configurable via `MODEL_CHOICE` env var (default: `gpt-4o-mini`)
  - Input: User query + RAG results (top 5 chunks)
  - Output: JSON with score, reasoning, gaps
  - Temperature: 0.3 (deterministic)

**Configuration**:
```bash
# Existing RAG enhancements (all respected)
USE_CONTEXTUAL_EMBEDDINGS=true
USE_HYBRID_SEARCH=true
USE_RERANKING=true
USE_AGENTIC_RAG=true

# New agentic search settings
AGENTIC_SEARCH_COMPLETENESS_THRESHOLD=0.8  # 0.0-1.0
```

---

### 2. Web Search (STAGE 2)

**Purpose**: Find promising URLs when local knowledge is insufficient

**Components**:
- **SearXNG Integration**: Uses existing `search()` tool
  - Returns URLs with titles and snippets
  - Configurable result count

- **LLM URL Ranker**:
  - Model: Same as completeness evaluator
  - Input: Query + information gaps + URL metadata (title, snippet)
  - Output: Ranked list with relevance scores (0.0-1.0)
  - Temperature: 0.3

- **URL Filter**:
  - Keep only URLs with score > threshold (default 0.7)
  - Limit to top N URLs (default 3)

**Configuration**:
```bash
AGENTIC_SEARCH_URL_SCORE_THRESHOLD=0.7     # Min score to crawl
AGENTIC_SEARCH_MAX_URLS_PER_ITERATION=3    # Max URLs to crawl
```

---

### 3. Selective Crawling & Indexing (STAGE 3)

**Purpose**: Crawl only promising URLs and index with full pipeline

**Components**:

#### 3.1 Crawling
- Uses existing `scrape_urls()` tool
- Parallel crawling (configurable concurrency)
- Respects all Crawl4AI settings

#### 3.2 Indexing Pipeline
**Uses existing `add_documents_to_database()` function** - no changes needed!

Pipeline stages (all automatic):
1. **Smart Chunking** (`smart_chunk_markdown`)
   - Respects code blocks (```)
   - Respects paragraphs (\n\n)
   - Respects sentences (.)
   - Configurable chunk size

2. **Contextual Embeddings** (if `USE_CONTEXTUAL_EMBEDDINGS=true`)
   - LLM generates context for each chunk
   - Parallel processing via ThreadPoolExecutor
   - Fallback to standard embeddings on error
   - Model: `CONTEXTUAL_EMBEDDING_MODEL` (default: gpt-4o-mini)

3. **Embedding Generation**
   - OpenAI `text-embedding-3-small`
   - Batch processing (20 chunks/batch)
   - Retry logic with exponential backoff

4. **Qdrant Storage**
   - Deterministic IDs (URL + chunk_number)
   - Automatic deduplication by URL
   - Metadata storage (url, chunk_number, source_id)
   - Sources table update

#### 3.3 Crawl Metadata Extraction (RESEARCH NEEDED)

**âš ï¸ TODO: Research Crawl4AI capabilities**

Questions to investigate:
1. Does `CrawlResult` object contain content summaries?
2. Can we extract structured metadata (topics, keywords)?
3. Is there a way to get page structure (headers, sections)?
4. Can we detect code blocks and their languages?

**Research sources**:
- Crawl4AI documentation: https://crawl4ai.com/docs
- `CrawlResult` class definition in codebase
- Existing usage in `services/crawling.py`

**Potential metadata to extract**:
```python
# If available in CrawlResult:
metadata = {
    "title": result.title,
    "description": result.description,
    "main_topics": result.topics,  # if available
    "headers": result.headers,     # if available
    "code_blocks": [
        {"language": "python", "count": 5},
        {"language": "javascript", "count": 3}
    ],
    "content_summary": result.summary  # if available
}
```

**Use case**: Pass metadata to LLM for generating smarter Qdrant queries

#### 3.4 Search Hints Generation (Optional Enhancement)

If metadata is available:
```python
# LLM generates optimal Qdrant queries based on crawled content
hints = llm.generate_search_hints(
    original_query="How to implement OAuth2 in FastAPI?",
    crawled_metadata={
        "title": "FastAPI Security Tutorial",
        "topics": ["OAuth2", "JWT", "authentication"],
        "code_blocks": [{"language": "python", "count": 8}]
    }
)

# Output:
# [
#   "FastAPI OAuth2PasswordBearer implementation",
#   "JWT token generation in FastAPI",
#   "FastAPI security dependencies"
# ]

# Then query Qdrant with each hint
for hint in hints:
    results = perform_rag_query(hint)
```

**Configuration**:
```bash
AGENTIC_SEARCH_USE_SEARCH_HINTS=true  # Enable smart query generation
```

---

### 4. Query Refinement (STAGE 4)

**Purpose**: Generate better queries when results are still incomplete

**Components**:
- **LLM Query Refiner**:
  - Model: Same as other LLM calls
  - Input: Original query + current query + information gaps
  - Output: 2-3 refined queries
  - Temperature: 0.5 (slightly more creative)

- **Iteration Controller**:
  - Tracks iteration count
  - Enforces max iterations limit
  - Prevents infinite loops

**Configuration**:
```bash
AGENTIC_SEARCH_MAX_ITERATIONS=3  # Max search-crawl cycles
```

---

## ğŸ“ MCP Tool Interface

```python
@mcp.tool()
async def agentic_search(
    ctx: Context,
    query: str,
    completeness_threshold: float = 0.8,
    max_iterations: int = 3,
    max_urls_per_iteration: int = 3,
    url_score_threshold: float = 0.7,
    use_search_hints: bool = False,
) -> str:
    """
    Intelligent iterative search with automatic refinement.
    
    Workflow:
    1. Check Qdrant for existing knowledge
    2. If incomplete, search web and rank URLs with LLM
    3. Crawl promising URLs selectively
    4. Re-evaluate completeness
    5. If still incomplete, refine query and repeat
    
    Args:
        query: User's search query
        completeness_threshold: Min score for answer completeness (0-1)
        max_iterations: Max search-crawl cycles (default: 3)
        max_urls_per_iteration: Max URLs to crawl per cycle (default: 3)
        url_score_threshold: Min relevance score to crawl URL (default: 0.7)
        use_search_hints: Generate smart Qdrant queries from metadata (default: False)
    
    Returns:
        JSON with:
        - success: bool
        - query: original query
        - iterations: number of cycles performed
        - completeness: final completeness score
        - results: RAG results from Qdrant
        - search_history: detailed log of actions taken
        - status: "complete" | "max_iterations_reached"
    """
```

**Response Format**:
```json
{
  "success": true,
  "query": "How to implement OAuth2 in FastAPI?",
  "iterations": 2,
  "completeness": 0.92,
  "results": [
    {
      "content": "...",
      "url": "https://fastapi.tiangolo.com/tutorial/security/",
      "similarity_score": 0.89,
      "chunk_index": 0
    }
  ],
  "search_history": [
    {
      "iteration": 1,
      "query": "How to implement OAuth2 in FastAPI?",
      "action": "local_check",
      "completeness": 0.45,
      "gaps": ["JWT token generation", "refresh tokens"]
    },
    {
      "iteration": 1,
      "action": "web_search",
      "urls_found": 10,
      "urls_ranked": 10,
      "promising_urls": 3
    },
    {
      "iteration": 1,
      "action": "crawl",
      "urls": [
        "https://fastapi.tiangolo.com/tutorial/security/",
        "https://realpython.com/fastapi-oauth2/",
        "https://auth0.com/blog/fastapi-authentication/"
      ],
      "urls_stored": 3,
      "chunks_stored": 45
    },
    {
      "iteration": 2,
      "query": "FastAPI JWT token generation and refresh",
      "action": "local_check",
      "completeness": 0.92,
      "gaps": []
    }
  ],
  "status": "complete"
}
```

---

## ğŸ”¬ Research Tasks

### Priority 1: Crawl4AI Metadata Extraction

**Goal**: Determine what metadata Crawl4AI can provide

**Tasks**:
1. Read Crawl4AI documentation
   - Focus on `CrawlResult` object
   - Look for content analysis features
   - Check for summarization capabilities

2. Examine existing code
   - Review `services/crawling.py`
   - Check what fields are currently used from `CrawlResult`
   - Look for unused fields that might contain metadata

3. Experiment with Crawl4AI
   - Test crawling a sample page
   - Inspect full `CrawlResult` object
   - Document available fields

**Deliverable**: Document listing available metadata fields and their use cases

---

### Priority 2: Search Hints Effectiveness

**Goal**: Determine if LLM-generated search hints improve results

**Tasks**:
1. Implement basic version without hints
2. Implement version with hints
3. Compare results on test queries
4. Measure:
   - Completeness scores
   - Number of iterations needed
   - User satisfaction (if possible)

**Deliverable**: Decision on whether to include search hints feature

---

## ğŸš€ Implementation Plan

### Phase 1: Core Pipeline (Week 1)
**Priority**: ğŸ”¥ Critical

**Tasks**:
1. Implement STAGE 1 (Local Knowledge Check)
   - LLM completeness evaluator
   - Integration with existing `perform_rag_query`
   - Unit tests

2. Implement STAGE 2 (Web Search)
   - LLM URL ranker
   - Integration with existing `search()` tool
   - URL filtering logic
   - Unit tests

3. Implement STAGE 3 (Selective Crawling)
   - Integration with existing `scrape_urls()`
   - Integration with existing indexing pipeline
   - Re-query logic
   - Unit tests

4. Implement STAGE 4 (Query Refinement)
   - LLM query refiner
   - Iteration controller
   - Unit tests

5. Integration Testing
   - End-to-end test with real queries
   - Test iteration limits
   - Test completeness thresholds

**Deliverable**: Working `agentic_search` tool with basic functionality

---

### Phase 2: Metadata Enhancement (Week 2)
**Priority**: âš ï¸ High

**Tasks**:
1. Research Crawl4AI metadata capabilities (see Research Tasks)
2. Implement metadata extraction (if available)
3. Implement search hints generation (if metadata available)
4. A/B testing: with vs without hints
5. Documentation

**Deliverable**: Enhanced `agentic_search` with metadata-driven queries

---

### Phase 3: Optimization & Monitoring (Week 3)
**Priority**: âš ï¸ Medium

**Tasks**:
1. Add detailed logging
2. Add performance metrics
   - Completeness scores over time
   - Crawl efficiency (URLs crawled vs total)
   - Cost tracking (LLM calls, embeddings)
3. Add configuration validation
4. Add error recovery
5. Documentation

**Deliverable**: Production-ready `agentic_search` with monitoring

---

## ğŸ“Š Configuration Reference

### Environment Variables

```bash
# ============================================
# AGENTIC SEARCH CONFIGURATION
# ============================================

# Core Settings
AGENTIC_SEARCH_ENABLED=true                      # Enable agentic search tool
AGENTIC_SEARCH_COMPLETENESS_THRESHOLD=0.8        # Min completeness score (0.0-1.0)
AGENTIC_SEARCH_MAX_ITERATIONS=3                  # Max search-crawl cycles
AGENTIC_SEARCH_MAX_URLS_PER_ITERATION=3          # Max URLs to crawl per cycle
AGENTIC_SEARCH_URL_SCORE_THRESHOLD=0.7           # Min URL relevance score

# Advanced Settings
AGENTIC_SEARCH_USE_SEARCH_HINTS=false            # Generate smart Qdrant queries
AGENTIC_SEARCH_LLM_TEMPERATURE=0.3               # LLM temperature for evaluation
AGENTIC_SEARCH_MAX_QDRANT_RESULTS=10             # Max results from Qdrant

# Existing RAG Settings (all respected)
USE_CONTEXTUAL_EMBEDDINGS=true                   # Use contextual embeddings
USE_HYBRID_SEARCH=true                           # Use hybrid search
USE_RERANKING=true                               # Use reranking
USE_AGENTIC_RAG=true                             # Use agentic RAG for code

# LLM Settings (shared)
MODEL_CHOICE=gpt-4o-mini                         # LLM model for all evaluations
OPENAI_API_KEY=sk-...                            # OpenAI API key
```

---

## ğŸ¯ Success Criteria

### Functional Requirements
- âœ… Check Qdrant before web search
- âœ… LLM evaluates answer completeness
- âœ… LLM ranks URLs by relevance
- âœ… Selective crawling (only promising URLs)
- âœ… Full indexing pipeline (all RAG enhancements)
- âœ… Iterative query refinement
- âœ… Configurable thresholds and limits

### Performance Requirements
- âœ… Completeness score >80% in 90% of queries
- âœ… <30% of search results crawled (vs 100% baseline)
- âœ… 50-70% cost reduction vs exhaustive crawling
- âœ… <60 seconds per iteration (average)

### Quality Requirements
- âœ… 80%+ test coverage
- âœ… Comprehensive error handling
- âœ… Detailed logging and monitoring
- âœ… Clear documentation

---

## ğŸ“š Related Documentation

- [Contextual Embeddings](CONTEXTUAL_EMBEDDINGS.md) - Enhanced RAG with LLM context
- [Multi-Language Parsing](MULTI_LANGUAGE_PARSING.md) - Code analysis across languages
- [Project Cleanup Plan](PROJECT_CLEANUP_PLAN.md) - Overall project roadmap

---

## ğŸ”„ Integration with Existing Features

### Respects All RAG Enhancements
- **Contextual Embeddings**: Automatically used if `USE_CONTEXTUAL_EMBEDDINGS=true`
- **Hybrid Search**: Automatically used if `USE_HYBRID_SEARCH=true`
- **Reranking**: Automatically used if `USE_RERANKING=true`
- **Agentic RAG**: Automatically used if `USE_AGENTIC_RAG=true`

### Uses Existing Tools
- `perform_rag_query()` - For Qdrant queries
- `search()` - For SearXNG integration
- `scrape_urls()` - For crawling
- `add_documents_to_database()` - For indexing

### No Breaking Changes
- All existing tools continue to work
- Agentic search is a new, optional tool
- Existing configurations are respected

---

## ğŸ¯ Priority in Project Roadmap

**STATUS: ğŸ”¥ HIGHEST PRIORITY**

This feature is designated as the **most important development direction** for the project because:

1. **Unique Value Proposition**: No other MCP server offers intelligent, iterative search
2. **Cost Efficiency**: Dramatically reduces crawling costs through selective crawling
3. **Quality Improvement**: LLM-driven decisions ensure high-quality results
4. **User Experience**: Comprehensive answers without manual iteration
5. **Competitive Advantage**: Positions this project as the most advanced RAG-MCP solution

**Recommendation**: Prioritize this over all other features in the backlog.

---

## ğŸ“ Notes

- Implementation details intentionally omitted (as requested)
- Focus on architecture and data flow
- Research tasks clearly marked
- Integration points with existing code identified
- No assumptions about Crawl4AI capabilities (marked for research)

---

**Last Updated**: 2025-11-05  
**Next Review**: After Phase 1 completion
