# Crawl4AI MCP Server Refactoring Guide

## Overview

This guide documents the refactoring of the monolithic `src/crawl4ai_mcp.py` file (2,998 lines) into a modular, maintainable structure following the Single Responsibility Principle and other best practices.

## Current Issues

1. **File Size**: At nearly 3,000 lines, the file violates the 300-400 line guideline
2. **Mixed Responsibilities**: Combines utilities, services, tools, and configuration in one file
3. **Poor Maintainability**: Difficult to navigate, test, and modify
4. **Testing Challenges**: Large file makes unit testing more complex

## Refactored Structure

```
src/
â”œâ”€â”€ core/                    # Core infrastructure
â”‚   â”œâ”€â”€ __init__.py         # Core exports
â”‚   â”œâ”€â”€ context.py          # Application context and lifecycle
â”‚   â”œâ”€â”€ decorators.py       # Request tracking and other decorators
â”‚   â”œâ”€â”€ exceptions.py       # Custom exceptions
â”‚   â”œâ”€â”€ logging.py          # Logging configuration
â”‚   â””â”€â”€ stdout_utils.py     # Stdout management utilities
â”‚
â”œâ”€â”€ utils/                   # Utility functions
â”‚   â”œâ”€â”€ __init__.py         # Utility exports
â”‚   â”œâ”€â”€ reranking.py        # Cross-encoder reranking
â”‚   â”œâ”€â”€ text_processing.py  # Text chunking and processing
â”‚   â”œâ”€â”€ url_helpers.py      # URL parsing and validation
â”‚   â””â”€â”€ validation.py       # Input validation functions
â”‚
â”œâ”€â”€ services/                # Business logic services
â”‚   â”œâ”€â”€ __init__.py         # Service exports
â”‚   â”œâ”€â”€ crawling.py         # Web crawling services
â”‚   â”œâ”€â”€ search.py           # Search functionality
â”‚   â”œâ”€â”€ scraping.py         # URL scraping services
â”‚   â””â”€â”€ smart_crawl.py      # Intelligent crawling logic
â”‚
â”œâ”€â”€ database/                # Database interactions
â”‚   â”œâ”€â”€ __init__.py         # Database exports
â”‚   â”œâ”€â”€ rag_queries.py      # RAG query functionality
â”‚   â””â”€â”€ sources.py          # Source management
â”‚
â”œâ”€â”€ knowledge_graph/         # Neo4j knowledge graph
â”‚   â”œâ”€â”€ __init__.py         # Knowledge graph exports
â”‚   â”œâ”€â”€ handlers.py         # Command handlers
â”‚   â”œâ”€â”€ queries.py          # Graph queries
â”‚   â”œâ”€â”€ repository.py       # Repository parsing
â”‚   â””â”€â”€ validation.py       # Script validation
â”‚
â”œâ”€â”€ tools/                   # MCP tool definitions
â”‚   â”œâ”€â”€ __init__.py         # Tool registration
â”‚   â”œâ”€â”€ search_tool.py      # Search tool
â”‚   â”œâ”€â”€ scrape_tool.py      # Scraping tool
â”‚   â””â”€â”€ ...                 # Other tools
â”‚
â”œâ”€â”€ main.py                  # Application entry point
â””â”€â”€ server.py               # FastMCP server configuration
```

## Migration Strategy

### Phase 1: Core Infrastructure âœ…

- Created `core/` directory with exceptions, logging, decorators, and context management
- Extracted `MCPToolError`, `SuppressStdout`, `track_request`, and `crawl4ai_lifespan`
- Established centralized logging configuration

### Phase 2: Utilities âœ…

- Created `utils/` directory with validation, text processing, URL helpers, and reranking
- Extracted all utility functions that don't depend on external services
- Maintained backward compatibility with imports

### Phase 3: Services âœ…

- Created `services/` directory for business logic
- Extracted crawling services as the first module
- Prepared structure for search, scraping, and smart crawl services

### Phase 4: Database & Knowledge Graph âœ…

- Created `database/` modules for RAG queries and source management
- Created `knowledge_graph/` modules for Neo4j operations
- Successfully separated vector database and graph database operations
- Updated all tool functions to use the refactored modules
- Removed redundant code and improved modularity

### Phase 5: Configuration Management âœ…

- Created `config/` directory with settings management
- Extracted all environment variable usage to centralized configuration
- Implemented settings class with property accessors for all config values
- Updated all modules to use the configuration module instead of direct os.getenv() calls
- Added validation and default values in the configuration module
- Tested configuration loading and accessibility

### Phase 6: Tools and Testing âœ…

Implemented the tool registration pattern to work with FastMCP's architecture:

1. **Tool Registration Pattern** (Implemented):
   - Created `src/tools.py` with all tool definitions
   - Tools import implementations from service modules
   - Registration function `register_tools(mcp)` called in main.py
   - Maintains FastMCP compatibility while keeping modular structure

2. **Service Layer Expansion**:
   - Created `services/search.py` for SearXNG integration
   - Created `services/smart_crawl.py` for intelligent crawling
   - All business logic separated from tool definitions

3. **Testing Infrastructure**:
   - Created comprehensive unit tests for all modules
   - Added performance benchmarks
   - Verified import times and memory usage
   - Confirmed no circular dependencies

4. **Quality Metrics Achieved**:
   - Most files under 400 lines (tools.py exception due to FastMCP)
   - Module import times < 500ms each
   - Total startup time < 1 second
   - Memory footprint increase < 100MB

## Benefits Achieved

1. **Improved Maintainability**: Each file has a single, clear purpose
2. **Better Testing**: Smaller modules are easier to unit test
3. **Enhanced Readability**: Developers can quickly find specific functionality
4. **Scalability**: Easy to add new features without bloating existing files
5. **Performance**: Smaller files load faster and use less memory

## Test Coverage Status

### Current Coverage Analysis (Post-Refactoring)

**Overall Coverage**: ~20% (Critical gaps in new modules)

| Module | Files | Current Coverage | Test Files | Status |
|--------|-------|-----------------|------------|---------|
| **database/** | 8 | ~60% | 15+ test files | âš ï¸ Partial |
| **core/** | 7 | ~15% | test_refactored_modules.py | âŒ Minimal |
| **utils/** | 5 | ~20% | test_refactored_modules.py | âŒ Minimal |
| **services/** | 4 | ~5% | Import tests only | âŒ Critical Gap |
| **knowledge_graph/** | 5 | ~5% | Import tests only | âŒ Critical Gap |
| **config/** | 2 | ~30% | test_refactored_modules.py | âš ï¸ Basic |
| **tools.py** | 1 | ~10% | Various MCP tests | âŒ Critical Gap |
| **main.py** | 1 | ~15% | Basic tests | âŒ Minimal |

### Critical Testing Gaps

- **Services Module**: Core business logic (crawling, search, smart crawl) largely untested
- **Tools Module**: MCP tool definitions lack comprehensive unit tests
- **Knowledge Graph**: AI validation features only have import tests
- **Core Infrastructure**: Decorators and stdout utilities missing tests

For detailed unit testing plan, see: `tests/plans/UNIT_TESTING_PLAN.md`

## Testing Strategy

1. **Unit Tests**: Create tests for each module independently
2. **Integration Tests**: Test module interactions
3. **Regression Tests**: Ensure functionality remains unchanged
4. **Coverage Goals**: Aim for 80%+ coverage per module

## Rollback Plan

If issues arise during migration:

1. Keep original `crawl4ai_mcp.py` as backup
2. Use feature flags to switch between old/new implementations
3. Gradual migration allows partial rollback

## Phase 7: main.py Refactoring (CRITICAL - IN PROGRESS)

### Current Issues in main.py

**File**: `src/main.py` (419 lines)

**Critical Problems**:
1. **Browser Process Leak**: Double lifespan initialization (lines 34 + 75)
   - FastMCP calls lifespan automatically via `run_http_async()`
   - Manual `async with crawl4ai_lifespan(mcp)` creates duplicate crawlers
   - Result: 7 crawler instances in 38 minutes, 300+ browser processes, 99% memory
   - See: `BROWSER_LEAK_ROOT_CAUSE.md`

2. **Architecture Violation**: 300 lines of OAuth2 setup inside `main()` function
   - 6 OAuth2 routes defined as nested functions (lines 143-384)
   - 10 levels of nesting (critical complexity)
   - Duplicate code: `src/auth/routes.py` already has these functions but unused
   - Middleware setup mixed with route definitions

3. **Maintainability**: 366-line `main()` function (should be ~50 lines)

### Refactoring Plan

#### Step 1: Extract OAuth2 Setup Module (2-3 hours)

**Create**: `src/auth/setup.py`

```python
def setup_oauth2_routes(mcp: FastMCP, oauth2_server: OAuth2Server, 
                        host: str, port: str) -> None:
    """Register OAuth2 routes with FastMCP server."""
    from auth.routes import (
        authorization_server_metadata,
        protected_resource_metadata,
        register_client,
        authorize_get,
        authorize_post,
        token_endpoint
    )
    
    resource_url = f"https://{host}:{port}" if host != "0.0.0.0" else settings.oauth2_issuer
    
    @mcp.custom_route("/.well-known/oauth-authorization-server", methods=["GET"])
    async def _metadata(request):
        return await authorization_server_metadata(request, oauth2_server)
    
    # ... register other routes
```

**Benefits**:
- Reuses existing `src/auth/routes.py` functions
- Removes 240 lines from `main()`
- Single responsibility: OAuth2 route registration

#### Step 2: Extract Middleware Setup (1 hour)

**Create**: `src/middleware/setup.py`

```python
def setup_middleware(use_oauth2: bool, oauth2_server: OAuth2Server = None) -> List[Middleware]:
    """Configure authentication middleware based on settings."""
    middleware = []
    
    if use_oauth2:
        from starlette.middleware.sessions import SessionMiddleware
        from auth import DualAuthMiddleware
        
        middleware.append(Middleware(SessionMiddleware, secret_key=settings.oauth2_secret_key))
        middleware.append(Middleware(DualAuthMiddleware, oauth2_server=oauth2_server))
        logger.info("âœ“ OAuth2 + API Key dual authentication enabled")
    elif settings.mcp_api_key:
        middleware.append(Middleware(APIKeyMiddleware))
        logger.info("âœ“ API Key authentication enabled")
    
    return middleware
```

**Benefits**:
- Removes 60 lines from `main()`
- Testable in isolation
- Clear separation of concerns

#### Step 3: Fix Browser Leak - Remove Manual Lifespan (30 min)

**Before** (lines 67-401):
```python
if transport == "http":
    async with crawl4ai_lifespan(mcp) as context:  # âŒ CAUSES LEAK
        middleware = []
        # ... 300 lines ...
        await mcp.run_http_async(...)
```

**After**:
```python
if transport == "http":
    middleware = setup_middleware(settings.use_oauth2, oauth2_server)
    
    if settings.use_oauth2:
        setup_oauth2_routes(mcp, oauth2_server, host, port)
    
    await mcp.run_http_async(transport="http", host=host, port=int(port), middleware=middleware)
    # FastMCP automatically calls crawl4ai_lifespan ONCE âœ…
```

**Benefits**:
- Fixes browser process leak (single crawler instance)
- Removes 326 lines from `main()`
- Reduces nesting from 10 to 3 levels

#### Step 4: Simplify main() Function (30 min)

**Target structure** (~50 lines):
```python
async def main():
    try:
        logger.info("Main function started")
        transport = settings.transport.lower()
        
        sys.stdout.flush()
        sys.stderr.flush()
        
        if transport == "http":
            middleware = setup_middleware(settings.use_oauth2)
            if settings.use_oauth2:
                oauth2_server = OAuth2Server(settings.oauth2_issuer, settings.oauth2_secret_key)
                setup_oauth2_routes(mcp, oauth2_server, host, port)
            await mcp.run_http_async(transport="http", host=host, port=int(port), middleware=middleware)
        elif transport == "sse":
            await mcp.run_sse_async()
        else:
            await mcp.run_stdio_async()
    except Exception as e:
        logger.error(f"Error in main function: {e}")
        raise
```

#### Step 5: Integration Testing (2 hours)

**Test scenarios**:
1. HTTP mode with OAuth2 - verify single crawler initialization
2. HTTP mode with API key only
3. SSE mode
4. STDIO mode
5. Browser process monitoring (should stay at 8-10 processes)
6. Memory usage (should stay ~600MB)

**Success criteria**:
- âœ… Only 1 "Initializing AsyncWebCrawler" log entry
- âœ… Browser processes stable at 8-10
- âœ… Memory stable at ~600MB (not 4GB)
- âœ… All OAuth2 flows working
- âœ… Can run indefinitely without restart

### Estimated Time

- Step 1 (OAuth2 setup): 2-3 hours
- Step 2 (Middleware): 1 hour
- Step 3 (Fix leak): 30 min
- Step 4 (Simplify main): 30 min
- Step 5 (Testing): 2 hours
- **Total**: 6-7 hours

### Risk Assessment

**Risk Level**: Medium

**Risks**:
- OAuth2 route registration might behave differently when extracted
- Middleware order matters - must preserve exact sequence
- FastMCP lifespan behavior needs verification

**Mitigation**:
- Test each step independently
- Keep git commits small and atomic
- Monitor logs for crawler initialization count
- Rollback plan: revert to current main.py if issues

### Files to Modify

- `src/main.py` - Remove 326 lines, simplify to ~90 lines
- `src/auth/setup.py` - NEW (route registration)
- `src/middleware/setup.py` - NEW (middleware configuration)
- `src/auth/routes.py` - Already exists, no changes needed
- `tests/test_main_refactoring.py` - NEW (integration tests)

### Status

- â³ **Phase 7 - Pending**: Awaiting approval to proceed
- ğŸ“‹ **Dependencies**: None (can start immediately)
- ğŸ¯ **Priority**: CRITICAL (fixes production memory leak)

## Next Steps

1. **IMMEDIATE**: Execute Phase 7 refactoring (fixes browser leak + architecture)
2. Complete remaining service extractions
3. Increase test coverage to 80%+
4. Deploy and monitor performance
5. Document lessons learned

## Notes

- The refactoring maintains all existing functionality
- No breaking changes to the MCP protocol or tool interfaces
- Performance should improve due to better code organization
- Future features can be added more easily in the modular structure
- **Phase 7 is critical**: Fixes production issue AND improves architecture
